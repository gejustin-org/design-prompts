# AI Code Generation Patterns & Caching Research

**Date:** 2026-01-28  
**Purpose:** Evaluate libraries, patterns, and approaches for reliable AI code generation with caching

---

## Executive Summary

For AI code generation with caching, the recommended stack is:

| Component | Recommendation | Rationale |
|-----------|----------------|-----------|
| **SDK** | **Vercel AI SDK** or **Instructor** | Provider-agnostic, excellent structured output, TypeScript-first |
| **Caching** | Content-addressable + provider-native | Combine prompt prefixes with local hash-based caching |
| **Validation** | Zod/Pydantic schemas | Built-in retry, streaming support |
| **Temperature** | 0.0 for determinism | Required for reproducible output |

---

## 1. LangChain

### Overview
LangChain is a framework for building LLM-powered applications. Primarily focused on agents and workflows.

### License & Cost
- **License:** MIT
- **Cost:** Free (open source)

### Relevant Features
- `create_agent()` handles structured output automatically
- Supports Pydantic models, dataclasses, TypedDict, JSON Schema
- Two strategies for structured output:
  - **ProviderStrategy** — Uses native provider structured output (OpenAI, Anthropic, xAI)
  - **ToolStrategy** — Falls back to tool calling for other models
- Automatic error handling and retry on validation failures
- Custom error handlers via `handle_errors` parameter

### Code Example
```python
from langchain.agents import create_agent
from langchain.agents.structured_output import ToolStrategy
from pydantic import BaseModel, Field

class CodeOutput(BaseModel):
    code: str = Field(description="Generated code")
    language: str = Field(description="Programming language")
    explanation: str = Field(description="Brief explanation")

agent = create_agent(
    model="claude-sonnet-4-5-20250929",
    tools=[],
    response_format=ToolStrategy(
        schema=CodeOutput,
        handle_errors=True  # Auto-retry on validation failure
    )
)
```

### Integration Assessment
- **Pros:** Mature ecosystem, good abstractions, supports multiple providers
- **Cons:** Heavy framework, may be overkill for focused code generation
- **Verdict:** Consider if building agents, skip for simple code generation

---

## 2. Vercel AI SDK

### Overview
TypeScript-first SDK for AI applications. Excellent for web/Node.js environments.

### License & Cost
- **License:** Apache 2.0
- **Cost:** Free (open source)

### Relevant Features
- **Output.object()** — Structured output with Zod schema validation
- **Output.array()** — Stream arrays element by element
- **Output.choice()** — Classification/enum responses
- Streaming support with `streamText()` and `partialOutputStream`
- Property descriptions via `.describe()` for better LLM understanding
- Built-in error handling with `NoObjectGeneratedError`
- Combines structured output with tool calling in same request

### Code Example
```typescript
import { generateText, Output } from 'ai';
import { z } from 'zod';

const { output } = await generateText({
  model: "anthropic/claude-sonnet-4.5",
  output: Output.object({
    name: 'GeneratedCode',
    description: 'Code generated by AI',
    schema: z.object({
      code: z.string().describe('The generated source code'),
      language: z.enum(['typescript', 'python', 'javascript']),
      imports: z.array(z.string()).describe('Required imports'),
      explanation: z.string().optional(),
    }),
  }),
  prompt: 'Generate a function that...',
});
```

### Integration Assessment
- **Pros:** TypeScript-native, excellent DX, Zod integration, streaming
- **Cons:** TypeScript only (no Python)
- **Verdict:** ⭐ **Top choice for TypeScript projects**

---

## 3. Instructor

### Overview
Most popular Python library for structured LLM output (3M+ downloads/month, 11k stars).

### License & Cost
- **License:** MIT
- **Cost:** Free (open source)

### Relevant Features
- **Pydantic-based** — Full validation, type safety, IDE support
- **Automatic retries** — Built-in retry on validation failure (`max_retries`)
- **15+ providers** — OpenAI, Anthropic, Google, Ollama, DeepSeek, etc.
- **Streaming** — `create_partial()` for partial objects, `create_iterable()` for lists
- **Hooks system** — Intercept events for logging/monitoring
- **Unified API** — `from_provider()` works identically across all providers

### Code Example
```python
import instructor
from pydantic import BaseModel, Field
from typing import List

class GeneratedCode(BaseModel):
    code: str = Field(..., description="The generated source code")
    language: str = Field(..., pattern=r"^(python|typescript|javascript)$")
    imports: List[str] = Field(default_factory=list)
    
    @field_validator('code')
    def validate_not_empty(cls, v):
        if not v.strip():
            raise ValueError('Code cannot be empty')
        return v

client = instructor.from_provider("anthropic/claude-sonnet-4-5-20250929")

result = client.create(
    response_model=GeneratedCode,
    messages=[{"role": "user", "content": "Generate a function that..."}],
    max_retries=3  # Auto-retry on validation failure
)
```

### Integration Assessment
- **Pros:** Minimal, focused, Pydantic integration, multi-provider
- **Cons:** Python only
- **Verdict:** ⭐ **Top choice for Python projects**

---

## 4. Anthropic SDK

### Overview
Official Python SDK for Claude models.

### License & Cost
- **License:** MIT
- **Cost:** Free SDK; API usage priced per token

### Relevant Features
- **Streaming** — `client.messages.stream()` with text_stream helper
- **Tool helpers** — `@beta_tool` decorator for function calling
- **Token counting** — `count_tokens()` before API call
- **Message batches** — Batch processing for bulk operations
- **AWS Bedrock** — Built-in support via `AnthropicBedrock`
- **Google Vertex** — Built-in support via `AnthropicVertex`

### Prompt Caching (Anthropic-Native)
Anthropic's prompt caching is automatic for prompts ≥1024 tokens:
- Place static content (system prompts, examples) at beginning
- Variable content at end
- Cache persists 5-10 minutes of inactivity (up to 1 hour)

### Code Example
```python
from anthropic import Anthropic

client = Anthropic()

# For structured output, use tool calling pattern
response = client.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=1024,
    tools=[{
        "name": "generate_code",
        "description": "Generate code based on specification",
        "input_schema": {
            "type": "object",
            "properties": {
                "code": {"type": "string"},
                "language": {"type": "string"},
            },
            "required": ["code", "language"]
        }
    }],
    messages=[{"role": "user", "content": "Generate a function..."}]
)
```

### Integration Assessment
- **Pros:** Direct access, official support, full feature set
- **Cons:** Anthropic-only, less structured output convenience
- **Verdict:** Use directly for Anthropic-specific features; wrap with Instructor for structured output

---

## 5. OpenAI SDK

### Overview
Official Python/TypeScript SDK for OpenAI models.

### License & Cost
- **License:** MIT
- **Cost:** Free SDK; API usage priced per token

### Relevant Features
- **Structured Outputs** — Native JSON Schema enforcement (gpt-4o+)
- **Pydantic/Zod helpers** — `responses.parse()` with schema
- **Streaming** — Full streaming support with partial parsing
- **Refusal handling** — Programmatic detection of safety refusals

### Prompt Caching (OpenAI-Native)
OpenAI's caching is automatic:
- Minimum 1024 tokens
- Cache routing based on prefix hash (first ~256 tokens)
- `prompt_cache_key` parameter for consistent routing
- `prompt_cache_retention` — `in_memory` (5-10 min) or `24h` (extended)
- Up to 80% latency reduction, 90% cost reduction

### Code Example
```python
from openai import OpenAI
from pydantic import BaseModel

client = OpenAI()

class CalendarEvent(BaseModel):
    name: str
    date: str
    participants: list[str]

response = client.responses.parse(
    model="gpt-4o-2024-08-06",
    input=[
        {"role": "system", "content": "Extract event information."},
        {"role": "user", "content": "Alice and Bob are going..."},
    ],
    text_format=CalendarEvent,
)

event = response.output_parsed  # Typed CalendarEvent
```

### Integration Assessment
- **Pros:** Native structured output, excellent TypeScript support
- **Cons:** OpenAI-only
- **Verdict:** Use for OpenAI-specific projects; otherwise use abstraction layer

---

## 6. Prompt Caching Strategies

### Provider-Native Caching

| Provider | Min Tokens | Retention | Cost Reduction |
|----------|------------|-----------|----------------|
| **OpenAI** | 1024 | 5-10 min (in-memory), 24h (extended) | Up to 90% |
| **Anthropic** | 1024 | 5-10 min (up to 1 hour) | Up to 90% |

**Best Practices:**
1. Structure prompts with static content first (system prompt, examples)
2. Put variable/user content at the end
3. Use consistent prefix lengths
4. Keep request rate <15/min per unique prefix (OpenAI)

### Content-Addressable Caching (Application-Level)

For deterministic code generation, implement your own caching layer:

```typescript
import crypto from 'crypto';

interface CacheKey {
  prompt: string;
  model: string;
  temperature: number;
  schema: object;
}

function generateCacheKey(input: CacheKey): string {
  const normalized = JSON.stringify({
    prompt: input.prompt.trim(),
    model: input.model,
    temperature: input.temperature,
    schema: input.schema,
  });
  return crypto.createHash('sha256').update(normalized).digest('hex');
}

// Cache implementation
const cache = new Map<string, CachedResponse>();

async function generateWithCache(input: CacheKey): Promise<Response> {
  const key = generateCacheKey(input);
  
  if (cache.has(key)) {
    return cache.get(key)!;
  }
  
  const response = await generateCode(input);
  cache.set(key, response);
  return response;
}
```

### Hybrid Caching Strategy (Recommended)

```
┌─────────────────────────────────────────────────────────────┐
│                     Request Flow                            │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  [User Request] ──► [Hash Generator] ──► [Local Cache]     │
│                           │                    │            │
│                           │                    ▼            │
│                           │              Hit? Return        │
│                           │                    │            │
│                           ▼                    │            │
│                    [LLM Request]◄──── Miss ────┘            │
│                           │                                 │
│                           ▼                                 │
│              [Provider Prompt Cache]                        │
│              (Anthropic/OpenAI native)                      │
│                           │                                 │
│                           ▼                                 │
│                    [Validation]                             │
│                           │                                 │
│                           ▼                                 │
│                  [Store in Local Cache]                     │
│                           │                                 │
│                           ▼                                 │
│                     [Return]                                │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 7. AI Code Validation Patterns

### Schema Validation (Required)

Always validate AI output against a schema:

```python
# Pydantic example
from pydantic import BaseModel, validator

class GeneratedCode(BaseModel):
    code: str
    language: str
    
    @validator('code')
    def validate_syntax(cls, v, values):
        language = values.get('language', 'python')
        if language == 'python':
            try:
                compile(v, '<string>', 'exec')
            except SyntaxError as e:
                raise ValueError(f'Invalid Python syntax: {e}')
        return v
```

### AST Validation

For code generation, validate syntax using AST:

```python
import ast

def validate_python_code(code: str) -> tuple[bool, str]:
    try:
        ast.parse(code)
        return True, ""
    except SyntaxError as e:
        return False, f"Line {e.lineno}: {e.msg}"
```

### Execution Sandbox (Optional)

For high-stakes code, consider execution validation:

```python
import subprocess
import tempfile

def validate_code_execution(code: str, test_cases: list) -> bool:
    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
        f.write(code)
        f.flush()
        
        result = subprocess.run(
            ['python', f.name],
            capture_output=True,
            timeout=5,
            text=True
        )
        return result.returncode == 0
```

### Retry Pattern

Implement automatic retry on validation failure:

```python
async def generate_with_retry(
    prompt: str,
    schema: type,
    max_retries: int = 3
) -> BaseModel:
    errors = []
    
    for attempt in range(max_retries):
        try:
            result = await generate(prompt, schema)
            return result
        except ValidationError as e:
            errors.append(str(e))
            # Append error to prompt for self-correction
            prompt += f"\n\nPrevious attempt failed: {e}. Please fix."
    
    raise ExhaustedRetriesError(errors)
```

---

## 8. Temperature & Reproducibility

### Best Practices

| Setting | Value | Use Case |
|---------|-------|----------|
| `temperature` | **0.0** | Deterministic code generation |
| `temperature` | 0.3-0.7 | Creative variations |
| `top_p` | 1.0 | Default (don't modify with temp=0) |
| `seed` | Fixed integer | OpenAI reproducibility (beta) |

### Deterministic Code Generation

```python
# Anthropic
response = client.messages.create(
    model="claude-sonnet-4-5-20250929",
    temperature=0.0,  # Deterministic
    max_tokens=1024,
    messages=[...]
)

# OpenAI
response = client.responses.create(
    model="gpt-4o",
    temperature=0.0,
    seed=42,  # Additional reproducibility
    input=[...]
)
```

### Caveats
- Even with `temperature=0`, minor variations can occur
- Model updates can change outputs
- Cache identical prompts for true reproducibility

---

## 9. Existing AI Codegen Tools

### Aider (Open Source)

**URL:** https://github.com/Aider-AI/aider  
**License:** Apache 2.0

**Key Features:**
- **Repository Map** — Graph-based analysis of entire codebase
- **Git Integration** — Automatic commits with sensible messages
- **Multi-model** — Claude, GPT, DeepSeek, local models
- **100+ Languages** — Python, TypeScript, Rust, Go, etc.
- **Linting/Testing** — Automatic validation after changes

**Interesting Patterns:**
1. **Repo Map** — Sends concise map of codebase to LLM
   - Key classes, functions, signatures
   - Graph ranking to select most relevant code
   - Dynamic token budget based on chat state

2. **Edit Formats** — Multiple strategies for code edits:
   - Whole file replacement
   - Search/replace blocks
   - Unified diffs

### Continue (Open Source)

**URL:** https://github.com/continuedev/continue  
**License:** Apache 2.0

**Key Features:**
- CLI-based AI coding agent
- VS Code and JetBrains extensions
- Cloud agents for async workflows (PR reviews, triage)
- Headless mode for CI/CD integration

### Cursor (Proprietary)

- IDE with built-in AI coding
- Context rules for project-specific behavior
- Closed source, commercial product
- Notable for UX patterns worth studying

---

## 10. Recommendations

### SDK Selection

| Use Case | Recommendation |
|----------|----------------|
| **TypeScript project** | Vercel AI SDK |
| **Python project** | Instructor |
| **Multi-provider flexibility** | Instructor (Python) or AI SDK (TS) |
| **Anthropic-only** | Anthropic SDK + Instructor wrapper |
| **OpenAI-only** | OpenAI SDK native structured output |

### Architecture

```
┌────────────────────────────────────────────────────────┐
│                  AI Code Generation Service            │
├────────────────────────────────────────────────────────┤
│                                                        │
│  ┌──────────────┐  ┌──────────────┐  ┌─────────────┐ │
│  │   Prompts    │  │   Schemas    │  │   Cache     │ │
│  │   (static)   │  │   (Zod/Py)   │  │   (Redis)   │ │
│  └──────┬───────┘  └──────┬───────┘  └──────┬──────┘ │
│         │                 │                  │        │
│         ▼                 ▼                  ▼        │
│  ┌──────────────────────────────────────────────────┐│
│  │           Request Handler                         ││
│  │  1. Generate cache key                           ││
│  │  2. Check local cache                            ││
│  │  3. Call LLM if miss                             ││
│  │  4. Validate response                            ││
│  │  5. Store in cache                               ││
│  └──────────────────────────────────────────────────┘│
│                          │                            │
│                          ▼                            │
│  ┌──────────────────────────────────────────────────┐│
│  │     Provider Layer (Instructor / AI SDK)          ││
│  │  - Structured output with schema                  ││
│  │  - Automatic retry on validation failure          ││
│  │  - Streaming support                              ││
│  └──────────────────────────────────────────────────┘│
│                          │                            │
│                          ▼                            │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐          │
│  │ Anthropic│  │  OpenAI  │  │  Others  │          │
│  └──────────┘  └──────────┘  └──────────┘          │
│                                                        │
└────────────────────────────────────────────────────────┘
```

### Implementation Checklist

- [ ] **Schema Definition** — Define Zod/Pydantic schemas for all code outputs
- [ ] **Temperature** — Set to 0.0 for deterministic generation
- [ ] **Validation** — AST parsing + schema validation + optional sandbox
- [ ] **Retry Logic** — 3 retries with error feedback in prompt
- [ ] **Caching Layer** — Content-addressable cache (Redis/SQLite)
- [ ] **Provider Abstraction** — Use Instructor/AI SDK for flexibility
- [ ] **Prompt Structure** — Static system prompt first, variable content last
- [ ] **Monitoring** — Log cache hits, validation failures, retries

### Prompt Engineering Patterns

1. **System Prompt Structure:**
```
You are a code generation assistant.
[Static rules and constraints - cacheable]

Output Format:
- Respond ONLY with valid JSON matching the schema
- Include all required fields
- Use proper syntax for the target language

[Examples of good output - cacheable]
```

2. **Few-Shot Examples:**
```
Example 1:
Input: "Create a function to calculate factorial"
Output: {"code": "def factorial(n)...", "language": "python"}

Example 2:
...
```

3. **Self-Correction Prompt:**
```
Previous attempt failed validation: {error}
Please fix the following issues:
1. {specific_issue_1}
2. {specific_issue_2}

Generate a corrected response:
```

---

## Summary Table

| Library | License | Language | Structured Output | Caching | Best For |
|---------|---------|----------|-------------------|---------|----------|
| **Vercel AI SDK** | Apache 2.0 | TypeScript | ⭐⭐⭐ Excellent | Provider-native | TS web apps |
| **Instructor** | MIT | Python | ⭐⭐⭐ Excellent | Provider-native | Python apps |
| **LangChain** | MIT | Python/TS | ⭐⭐ Good | Manual | Agents |
| **Anthropic SDK** | MIT | Python/TS | ⭐ Basic (tools) | Native | Claude-specific |
| **OpenAI SDK** | MIT | Python/TS | ⭐⭐⭐ Excellent | Native | GPT-specific |
| **Aider** | Apache 2.0 | Python | N/A (CLI tool) | N/A | Patterns to study |

---

## Next Steps

1. **Prototype** — Build minimal PoC with Instructor (Python) or AI SDK (TS)
2. **Benchmark** — Test cache hit rates and latency improvements
3. **Schema Library** — Define reusable schemas for common code patterns
4. **Validation Pipeline** — Implement AST validation for target languages
5. **Monitoring** — Add observability for cache performance and error rates
